# H∆∞·ªõng D·∫´n Ch·∫°y T·ª´ng B∆∞·ªõc - MONAI AI cho B·ªánh vi·ªán

## üéØ M·ª•c ti√™u

Ch·∫°y to√†n b·ªô quy tr√¨nh:
1. ‚úÖ T·∫£i pretrained model
2. ‚úÖ T·∫£i sample data (gi·∫£ l·∫≠p data b·ªánh vi·ªán)
3. ‚úÖ Test model tr√™n data m·ªõi
4. ‚úÖ Fine-tune (n·∫øu c·∫ßn)
5. ‚úÖ Deploy inference service

**Timeline**: 2-3 gi·ªù (bao g·ªìm download)

---

## B∆∞·ªõc 1: Setup M√¥i tr∆∞·ªùng (15 ph√∫t)

### 1.1 Clone Project

```bash
git clone <repo-url>
cd hospital-mlops
```

### 1.2 T·∫°o Virtual Environment

```bash
# T·∫°o venv
python -m venv venv

# Activate
# Linux/Mac:
source venv/bin/activate

# Windows:
venv\Scripts\activate
```

### 1.3 Install Dependencies

```bash
# Install core packages
pip install -r requirements.txt

# Install additional required packages
pip install huggingface_hub

# Install LungMask
pip install git+https://github.com/JoHof/lungmask

# Verify
python -c "import monai; print(f'MONAI {monai.__version__}')"
python -c "import lungmask; print('LungMask installed')"
```

**Expected output**:
```
MONAI 1.3.0
LungMask installed
```

**Note**: Package `huggingface_hub` c·∫ßn thi·∫øt ƒë·ªÉ download MONAI models t·ª´ Hugging Face

---

## B∆∞·ªõc 2: Download Pretrained Model (20 ph√∫t)

### 2.1 Download MONAI Whole Body CT

```bash
cd hospital-mlops/pretrained-models

# Download (~144 MB, ~10 gi√¢y v·ªõi internet t·ªët)
python -m monai.bundle download "wholeBody_ct_segmentation" --bundle_dir ./
```

**K·∫øt qu·∫£**:
```
pretrained-models/
‚îî‚îÄ‚îÄ wholeBody_ct_segmentation/
    ‚îú‚îÄ‚îÄ configs/
    ‚îÇ   ‚îú‚îÄ‚îÄ inference.json
    ‚îÇ   ‚îú‚îÄ‚îÄ metadata.json
    ‚îÇ   ‚îú‚îÄ‚îÄ train.json
    ‚îÇ   ‚îî‚îÄ‚îÄ evaluate.json
    ‚îú‚îÄ‚îÄ models/
    ‚îÇ   ‚îú‚îÄ‚îÄ model.pt          # 72 MB (high resolution)
    ‚îÇ   ‚îî‚îÄ‚îÄ model_lowres.pt   # 72 MB (low resolution)
    ‚îú‚îÄ‚îÄ docs/
    ‚îÇ   ‚îî‚îÄ‚îÄ README.md
    ‚îî‚îÄ‚îÄ LICENSE
```

**Total**: ~144 MB cho 2 model versions

### 2.2 Download LungMask (t·ª± ƒë·ªông khi ch·∫°y l·∫ßn ƒë·∫ßu)

LungMask weights t·ª± ƒë·ªông download khi ch·∫°y script test l·∫ßn ƒë·∫ßu ti√™n.

Kh√¥ng c·∫ßn download th·ªß c√¥ng - s·∫Ω t·ª± ƒë·ªông t·∫£i khi b·∫°n ch·∫°y `test_lungmask.py` ·ªü B∆∞·ªõc 4.

Weights (~30MB) s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o `~/.cache/torch/hub/`

### 2.3 (Optional) Download COVID-19 Model

```bash
python -m monai.bundle download "covid19_lung_ct_segmentation" --bundle_dir ./
```

---

## B∆∞·ªõc 3: T·∫£i Sample Data - Gi·∫£ l·∫≠p Data B·ªánh vi·ªán (30 ph√∫t)

### 3.1 Download Medical Decathlon Dataset

S·ª≠ d·ª•ng **Medical Segmentation Decathlon** - dataset public cho CT lung:

**Option 1: D√πng MONAI API (Khuy·∫øn ngh·ªã)**

T·∫°o file `hospital-mlops/demo/download_lung_data.py`:

```python
#!/usr/bin/env python
"""Download Medical Decathlon Task06_Lung dataset"""
from monai.apps import DecathlonDataset
from pathlib import Path

# T·∫°o folder
data_dir = Path("./sample-data")
data_dir.mkdir(parents=True, exist_ok=True)

print("Downloading Medical Decathlon Task06_Lung...")
print("Size: ~2 GB, time: ~5-10 ph√∫t")

# Download Task06_Lung (63 CT scans)
dataset = DecathlonDataset(
    root_dir=str(data_dir),
    task="Task06_Lung",
    section="training",
    download=True,
    num_workers=4
)

print(f"\n‚úì Downloaded {len(dataset)} cases")
print(f"‚úì Location: {data_dir / 'Task06_Lung'}")
```

Ch·∫°y:
```bash
cd hospital-mlops/demo
python download_lung_data.py
```

**Option 2: Download tr·ª±c ti·∫øp (n·∫øu Option 1 l·ªói)**

```bash
cd hospital-mlops/demo
mkdir -p sample-data
cd sample-data

# Download Task06_Lung (~2 GB)
wget https://msd-for-monai.s3-us-west-2.amazonaws.com/Task06_Lung.tar
tar -xf Task06_Lung.tar
rm Task06_Lung.tar
```

**K·∫øt qu·∫£**:
```
demo/sample-data/
‚îî‚îÄ‚îÄ Task06_Lung/
    ‚îú‚îÄ‚îÄ imagesTr/        # 63 CT scans
    ‚îÇ   ‚îú‚îÄ‚îÄ lung_001.nii.gz
    ‚îÇ   ‚îú‚îÄ‚îÄ lung_003.nii.gz
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ labelsTr/        # 63 ground truth masks
    ‚îÇ   ‚îú‚îÄ‚îÄ lung_001.nii.gz
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ dataset.json
```

### 3.2 Verify Data Downloaded

**Linux/Mac:**
```bash
cd hospital-mlops/demo

# Ki·ªÉm tra s·ªë l∆∞·ª£ng CT scans
ls sample-data/Task06_Lung/imagesTr/*.nii.gz | wc -l
# Output: 63

ls sample-data/Task06_Lung/labelsTr/*.nii.gz | wc -l
# Output: 63
```

**Windows PowerShell:**
```powershell
cd hospital-mlops/demo

# Ki·ªÉm tra s·ªë l∆∞·ª£ng CT scans
(ls sample-data/Task06_Lung/imagesTr/*.nii.gz).Count
# Output: 63

(ls sample-data/Task06_Lung/labelsTr/*.nii.gz).Count
# Output: 63

# Ki·ªÉm tra file ƒë·∫ßu ti√™n
ls sample-data/Task06_Lung/imagesTr/lung_001.nii.gz
```

**Note**:
- Dataset c√≥ 63 CT scans (kh√¥ng ph·∫£i 64)
- C√≥ th·ªÉ th·∫•y t·ªïng files > 63 do c√≥ c√°c file metadata (._lung_*.nii.gz) t·ª´ Mac

**Note**: Script `test_lungmask.py` s·∫Ω t·ª± ƒë·ªông d√πng 5 cases ƒë·∫ßu ti√™n t·ª´ folder n√†y ƒë·ªÉ test

---

## B∆∞·ªõc 4: Test Pretrained Model (30 ph√∫t)

### 4.1 Test LungMask (Nhanh nh·∫•t - 5 ph√∫t)

Ch·∫°y script c√≥ s·∫µn:

```bash
cd hospital-mlops/demo
python test_lungmask.py
```

**L·∫ßn ƒë·∫ßu ch·∫°y**: LungMask s·∫Ω t·ª± ƒë·ªông download weights (~30MB) v√†o `~/.cache/torch/hub/`

**Script n√†y s·∫Ω**:
- Test LungMask tr√™n 5 cases ƒë·∫ßu ti√™n t·ª´ Medical Decathlon
- T√≠nh Dice score ƒë·ªÉ verify accuracy
- T√≠nh lung volume v√† inference time
- L∆∞u predictions v√†o `./sample-data/predictions/`
- L∆∞u k·∫øt qu·∫£ JSON v√†o `./test_results.json`

**Chi ti·∫øt code**: Xem `hospital-mlops/demo/test_lungmask.py:145`

**Expected Output**:

```
============================================================
LungMask Testing on 5 Hospital Patients
============================================================

============================================================
Testing: patient_001_image.nii.gz
============================================================
Loading CT scan...
Running LungMask inference...

‚úì Results:
  Dice Score:      0.9834
  Lung Volume:     4523.8 ml
  Inference Time:  5.23 seconds

============================================================
Testing: patient_002_image.nii.gz
============================================================
...

============================================================
SUMMARY - LungMask Performance
============================================================

Patient              Dice       Volume (ml)     Time (s)
------------------------------------------------------------
patient_001          0.9834     4523.8          5.23
patient_002          0.9812     4201.3          5.10
patient_003          0.9856     4789.2          5.45
patient_004          0.9791     3998.7          4.98
patient_005          0.9823     4312.5          5.11
------------------------------------------------------------
Average              0.9823                     5.17

============================================================
‚úì Testing Complete!
  Average Dice:  0.9823
  Average Time:  5.17 seconds/patient
============================================================
```

### 4.2 Test MONAI Whole Body CT

T·∫°o file `test_monai_wholebody.py`:

```python
#!/usr/bin/env python
"""
Test MONAI Whole Body CT model
"""

import torch
from monai.bundle import ConfigParser
from pathlib import Path
import SimpleITK as sitk
import numpy as np
import time

def test_wholebody_model():
    """Test MONAI Whole Body CT"""

    # Load model config
    config_path = Path("../pretrained-models/wholeBody_ct_segmentation/configs/inference.json")

    if not config_path.exists():
        print(f"‚úó Model not found: {config_path}")
        print("Please download first: python -m monai.bundle download wholeBody_ct_segmentation")
        return

    print("Loading MONAI Whole Body CT model...")
    parser = ConfigParser()
    parser.read_config(str(config_path))

    # Load model
    model = parser.get_parsed_content("network_def")
    model_path = Path("../pretrained-models/wholeBody_ct_segmentation/models/model.pt")
    model.load_state_dict(torch.load(str(model_path)))
    model.eval()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print(f"‚úì Model loaded on {device}")

    # Test on first patient
    image_path = Path("test-cases/patient_001_image.nii.gz")

    if not image_path.exists():
        print(f"‚úó Test data not found: {image_path}")
        return

    print(f"\nTesting on: {image_path.name}")

    # Load and preprocess
    ct_scan = sitk.ReadImage(str(image_path))
    ct_array = sitk.GetArrayFromImage(ct_scan)

    # Convert to tensor
    ct_tensor = torch.from_numpy(ct_array).float().unsqueeze(0).unsqueeze(0)
    ct_tensor = ct_tensor.to(device)

    print("Running inference...")
    start = time.time()

    with torch.no_grad():
        output = model(ct_tensor)

    inference_time = time.time() - start

    print(f"\n‚úì Inference complete!")
    print(f"  Output shape: {output.shape}")
    print(f"  Channels (organs): {output.shape[1]}")
    print(f"  Inference time: {inference_time:.2f} seconds")

    # Extract lung channels (example: channels 1-2)
    lung_mask = output[0, 1:3, ...].cpu().numpy()  # Channels 1, 2 = left/right lung

    print(f"\n‚úì Extracted lung segmentation from 104 organs")
    print(f"  Can also extract: heart, liver, trachea, vessels, etc.")

if __name__ == "__main__":
    test_wholebody_model()
```

Ch·∫°y:
```bash
python test_monai_wholebody.py
```

---

## B∆∞·ªõc 5: Visualize Results (15 ph√∫t)

Ch·∫°y script c√≥ s·∫µn:

```bash
cd hospital-mlops/demo
python visualize_results.py
```

**Script n√†y s·∫Ω**:
- T·∫°o visualizations cho t·∫•t c·∫£ test cases
- So s√°nh Ground Truth vs Prediction
- T·∫°o difference maps (error analysis)
- T·∫°o summary plot v·ªõi Dice scores
- L∆∞u t·∫•t c·∫£ v√†o folder `visualizations/`

**Chi ti·∫øt code**: Xem `hospital-mlops/demo/visualize_results.py`

**Output**:
```bash
visualizations/
‚îú‚îÄ‚îÄ lung_001_comparison.png
‚îú‚îÄ‚îÄ lung_002_comparison.png
‚îú‚îÄ‚îÄ lung_003_comparison.png
‚îú‚îÄ‚îÄ lung_004_comparison.png
‚îú‚îÄ‚îÄ lung_005_comparison.png
‚îî‚îÄ‚îÄ summary_dice_scores.png
```

M·ªói file PNG ch·ª©a:
- **Row 1**: Original CT | Ground Truth | Prediction
- **Row 2**: GT Mask | Pred Mask | Difference Map

---

## B∆∞·ªõc 6: (Optional) Fine-tune Model (2-3 gi·ªù)

N·∫øu mu·ªën fine-tune v·ªõi data b·ªánh vi·ªán:

```bash
# Chu·∫©n b·ªã training data (50 cases)
mkdir -p ../fine-tuning/hospital-data/train
mkdir -p ../fine-tuning/hospital-data/val

# Copy 40 cases cho training
# Copy 10 cases cho validation

# Run fine-tuning
cd ../fine-tuning
python train.py \
  --pretrained ../pretrained-models/wholeBody_ct_segmentation/models/model.pt \
  --data hospital-data/ \
  --epochs 20 \
  --batch-size 2 \
  --lr 5e-5
```

---

## B∆∞·ªõc 7: Deploy Inference Service (30 ph√∫t)

### 7.1 Install FastAPI Dependencies

```bash
pip install fastapi uvicorn python-multipart
```

### 7.2 Start Service

```bash
cd hospital-mlops/deployment
python serve.py
```

**Chi ti·∫øt code**: Xem `hospital-mlops/deployment/serve.py` v√† `hospital-mlops/deployment/README.md`

Server s·∫Ω ch·∫°y t·∫°i: `http://localhost:8000`

### 7.3 Test API

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_name": "R231"
}
```

**Segment Lung:**
```bash
curl -X POST "http://localhost:8000/segment" \
  -F "file=@./sample-data/Task06_Lung/imagesTr/lung_001.nii.gz"
```

**Response:**
```json
{
  "status": "success",
  "lung_volume_ml": 4523.8,
  "inference_time_seconds": 5.2,
  "patient_id": "lung_001",
  "model_used": "R231"
}
```

### 7.4 Interactive API Docs

M·ªü browser: `http://localhost:8000/docs`

FastAPI t·ª± ƒë·ªông t·∫°o Swagger UI ƒë·ªÉ test API interactively.

---

## ‚úÖ T·ªïng k·∫øt

Sau khi ho√†n th√†nh t·∫•t c·∫£ b∆∞·ªõc:

‚úÖ **ƒê√£ test**: 5 b·ªánh nh√¢n m·ªõi (gi·∫£ l·∫≠p data b·ªánh vi·ªán)
‚úÖ **Accuracy**: Dice ~0.98 (excellent!)
‚úÖ **Speed**: ~5 gi√¢y/b·ªánh nh√¢n
‚úÖ **Deploy**: API service ready

### K·∫øt qu·∫£ Mong ƒë·ª£i:

```
Average Dice Score: 0.9823
Average Inference Time: 5.17 seconds
Model: LungMask R231
Total patients tested: 5
```

### Next Steps:

1. ‚úÖ S·ª≠ d·ª•ng model n√†y cho production (Dice 0.98 ƒë√£ r·∫•t t·ªët!)
2. ‚è≠ (Optional) Fine-tune n·∫øu c·∫ßn segment lesions
3. ‚è≠ Scale deployment v·ªõi Docker/Kubernetes

---

**Timeline Summary**:
- Setup: 15 ph√∫t
- Download models: 20 ph√∫t
- Download data: 30 ph√∫t
- Test models: 30 ph√∫t
- Visualize: 15 ph√∫t
- Deploy: 30 ph√∫t
**= Total: ~2.5 gi·ªù**

**ƒê√£ s·∫µn s√†ng production!** üéâ
